{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## forma\u00e7\u00e3o de precis\u00e3o mista"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Este m\u00f3dulo permite que os passes para a frente e para tr\u00e1s de sua rede neural a ser feito em FP16 (tamb\u00e9m conhecido como * metade precis\u00e3o *). Isto \u00e9 particularmente importante se voc\u00ea tiver uma GPU NVIDIA com [tensor cores](https://www.nvidia.com/en-us/data-center/tensorcore/), uma vez que pode acelerar o seu treinamento em 200% ou mais."]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [], "source": ["from fastai.gen_doc.nbdoc import *\n", "from fastai.callbacks.fp16 import *\n", "from fastai.vision import *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## vis\u00e3o global"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para treinar o seu modelo em precis\u00e3o mista voc\u00ea s\u00f3 tem que chamar [`Learner.to_fp16`](/train.html#to_fp16), que converte o modelo e modifica o [`Learner`](/basic_train.html#Learner) existente para adicionar [`MixedPrecision`](/callbacks.fp16.html#MixedPrecision)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h4 id=\"to_fp16\" class=\"doc_header\"><code>to_fp16</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/train.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#to_fp16-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4>\n", "\n", "> <code>to_fp16</code>(**`learn`**:[`Learner`](/basic_train.html#Learner), **`loss_scale`**:`float`=***`None`***, **`max_noskip`**:`int`=***`1000`***, **`dynamic`**:`bool`=***`True`***, **`clip`**:`float`=***`None`***, **`flat_master`**:`bool`=***`False`***, **`max_scale`**:`float`=***`16777216`***, **`loss_fp32`**:`bool`=***`True`***) \u2192 [`Learner`](/basic_train.html#Learner)\n", "\n", "<div class=\"collapse\" id=\"to_fp16-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#to_fp16-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>to_fp16</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Put `learn` in FP16 precision mode.  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(Learner.to_fp16)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Por exemplo:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["Total time: 00:03 <p><table style='width:300px; margin-bottom:10px'>\n", "  <tr>\n", "    <th>epoch</th>\n", "    <th>train_loss</th>\n", "    <th>valid_loss</th>\n", "    <th>accuracy</th>\n", "  </tr>\n", "  <tr>\n", "    <th>1</th>\n", "    <th>0.139469</th>\n", "    <th>0.115246</th>\n", "    <th>0.963199</th>\n", "  </tr>\n", "</table>\n"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["path = untar_data(URLs.MNIST_SAMPLE)\n", "data = ImageDataBunch.from_folder(path)\n", "model = simple_cnn((3,16,16,2))\n", "learn = Learner(data, model, metrics=[accuracy]).to_fp16()\n", "learn.fit_one_cycle(1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Os detalhes sobre a forma\u00e7\u00e3o de precis\u00e3o mista est\u00e3o dispon\u00edveis em [NVIDIA's documentation](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). Vamos apenas resumir o b\u00e1sico aqui.\n", "", "O \u00fanico par\u00e2metro que voc\u00ea pode querer ajustar \u00e9 `loss_scale`. Isto \u00e9 usado para dimensionar a perda, de modo que ele n\u00e3o estouro negativo FP16, levando \u00e0 perda de precis\u00e3o (esta \u00e9 revertida para o c\u00e1lculo final gradiente depois de converter de volta para fp32). Geralmente o padr\u00e3o `512` funciona bem, no entanto. Voc\u00ea tamb\u00e9m pode ativar ou desativar o achatamento do par\u00e2metro tensor principal com `flat_master = true`, no entanto, em nossa testar o diferente \u00e9 insignificante.\n", "", "Internamente, a chamada de retorno assegura que todos os par\u00e2metros do modelo (excepto camadas batchnorm, que requerem fp32) s\u00e3o convertidos para FP16, e uma c\u00f3pia fp32 tamb\u00e9m \u00e9 guardado. A c\u00f3pia fp32 (os `par\u00e2metros master`) \u00e9 o que \u00e9 usado para realmente atualizar com o otimizador; os par\u00e2metros FP16 s\u00e3o utilizados para calcular os gradientes. Isso ajuda a evitar underflow com taxas de aprendizagem pequenas.\n", "", "Tudo isso \u00e9 implementado pela seguinte chamada de retorno."]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h2 id=\"MixedPrecision\" class=\"doc_header\"><code>class</code> <code>MixedPrecision</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L64\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h2>\n", "\n", "> <code>MixedPrecision</code>(**`learn`**:[`Learner`](/basic_train.html#Learner), **`loss_scale`**:`float`=***`None`***, **`max_noskip`**:`int`=***`1000`***, **`dynamic`**:`bool`=***`True`***, **`clip`**:`float`=***`None`***, **`flat_master`**:`bool`=***`False`***, **`max_scale`**:`float`=***`16777216`***, **`loss_fp32`**:`bool`=***`True`***) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n", "\n", "<div class=\"collapse\" id=\"MixedPrecision-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>MixedPrecision</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Base class for creating callbacks for a [`Learner`](/basic_train.html#Learner).  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(MixedPrecision)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### m\u00e9todos de retorno de chamada"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Voc\u00ea n\u00e3o tem que chamar as seguintes fun\u00e7\u00f5es a si mesmo - eles s\u00e3o chamados pelo sistema [`Callback`](/callback.html#Callback) das fastai automaticamente para ativar a funcionalidade da classe."]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h4 id=\"MixedPrecision.on_backward_begin\" class=\"doc_header\"><code>on_backward_begin</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L93\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_backward_begin-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4>\n", "\n", "> <code>on_backward_begin</code>(**`last_loss`**:`Rank0Tensor`, **\\*\\*`kwargs`**:`Any`) \u2192 `Rank0Tensor`\n", "\n", "<div class=\"collapse\" id=\"MixedPrecision-on_backward_begin-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_backward_begin-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>on_backward_begin</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Scale gradients up by `self.loss_scale` to prevent underflow.  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(MixedPrecision.on_backward_begin)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h4 id=\"MixedPrecision.on_backward_end\" class=\"doc_header\"><code>on_backward_end</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L99\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_backward_end-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4>\n", "\n", "> <code>on_backward_end</code>(**\\*\\*`kwargs`**:`Any`)\n", "\n", "<div class=\"collapse\" id=\"MixedPrecision-on_backward_end-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_backward_end-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>on_backward_end</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Convert the gradients back to FP32 and divide them by the scale.  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(MixedPrecision.on_backward_end)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h4 id=\"MixedPrecision.on_loss_begin\" class=\"doc_header\"><code>on_loss_begin</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L89\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_loss_begin-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4>\n", "\n", "> <code>on_loss_begin</code>(**`last_output`**:`Tensor`, **\\*\\*`kwargs`**:`Any`) \u2192 `Tensor`\n", "\n", "<div class=\"collapse\" id=\"MixedPrecision-on_loss_begin-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_loss_begin-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>on_loss_begin</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Convert half precision output to FP32 to avoid reduction overflow.  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(MixedPrecision.on_loss_begin)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h4 id=\"MixedPrecision.on_step_end\" class=\"doc_header\"><code>on_step_end</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L118\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_step_end-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4>\n", "\n", "> <code>on_step_end</code>(**\\*\\*`kwargs`**:`Any`)\n", "\n", "<div class=\"collapse\" id=\"MixedPrecision-on_step_end-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_step_end-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>on_step_end</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Update the params from master to model and zero grad.  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(MixedPrecision.on_step_end)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": ["<h4 id=\"MixedPrecision.on_train_begin\" class=\"doc_header\"><code>on_train_begin</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L77\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_train_begin-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4>\n", "\n", "> <code>on_train_begin</code>(**\\*\\*`kwargs`**:`Any`)\n", "\n", "<div class=\"collapse\" id=\"MixedPrecision-on_train_begin-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#MixedPrecision-on_train_begin-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>on_train_begin</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div>\n", "\n", "Prepare the master model.  "], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["show_doc(MixedPrecision.on_train_begin)"]}], "metadata": {"jekyll": {"keywords": "fastai", "summary": "Training in mixed precision implementation", "title": "callbacks.fp16"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2}