{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## modelos de texto, dados e treinamento"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [], "source": ["from fastai.gen_doc.nbdoc import *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["O m\u00f3dulo [`text`](/text.html#text) da biblioteca fastai cont\u00e9m todas as fun\u00e7\u00f5es necess\u00e1rias para definir um conjunto de dados apropriado para as diversas tarefas NLP (Natural Language Processing) e rapidamente gerar modelos que voc\u00ea pode usar para eles. Especificamente:\n", "- [`text.transform`](/text.transform.html#text.transform) cont\u00e9m todos os scripts para pr\u00e9-processar seus dados, a partir de texto simples ao token ids,\n", "- [`text.data`](/text.data.html#text.data) cont\u00e9m a defini\u00e7\u00e3o de [`TextDataBunch`](/text.data.html#TextDataBunch), que a classe principal que voc\u00ea precisa em PNL,\n", "- [`text.learner`](/text.learner.html#text.learner) cont\u00e9m fun\u00e7\u00f5es auxiliares para criar rapidamente um modelo de linguagem ou um classificador RNN.\n", "", "D\u00ea uma olhada nos links acima para obter mais detalhes da API de cada m\u00f3dulo, de ler para uma vis\u00e3o geral r\u00e1pida."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## In\u00edcio R\u00e1pido: Treinando um modelo sentimento IMDb com * ULMFiT *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vamos come\u00e7ar com uma r\u00e1pida exemplo end-to-end de treinamento de um modelo. Vamos treinar um classificador sentimento em uma amostra dos dados populares IMDb, mostrando 4 etapas:\n", "", "1. Leitura e visualiza\u00e7\u00e3o dos dados IMDb\n", "1. Come\u00e7ar seus dados prontos para modelagem\n", "1. Ajustar um modelo de linguagem\n", "1. Construir um classificador"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Leitura e visualiza\u00e7\u00e3o dos dados da IMDb"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Primeiro vamos importar tudo o que precisamos para o texto."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from fastai.text import * "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ao contr\u00e1rio do que imagens em Computer Vision, texto pode n\u00e3o ser directamente transformada em n\u00fameros a ser alimentado em um modelo. A primeira coisa que precisamos fazer \u00e9 pr\u00e9-processar os dados para que mudar os textos brutos para listas de palavras ou tokens (um passo que \u00e9 chamado tokenization), em seguida, transformar essas fichas em n\u00fameros (um passo que \u00e9 chamado numericalization). Estes n\u00fameros s\u00e3o, ent\u00e3o, passados \u200b\u200bpara a incorpora\u00e7\u00e3o de camadas que vai convert\u00ea-los em matrizes de flutuadores antes de pass\u00e1-los atrav\u00e9s de um modelo.\n", "", "Voc\u00ea pode encontrar na web abund\u00e2ncia de [Word Embeddings](https://en.wikipedia.org/wiki/Word_embedding) para converter diretamente suas fichas em carros aleg\u00f3ricos. Essas incorpora\u00e7\u00f5es palavra t\u00eam geralmente ser treinado em um grande corpus como a wikipedia. Na sequ\u00eancia dos trabalhos de [ULMFiT](https://arxiv.org/abs/1801.06146), a biblioteca fastai \u00e9 mais focada sobre o uso pr\u00e9-treinados Modelos de Linguagem e afinar-los. embeddings Word s\u00e3o apenas vetores de 300 ou 400 carros aleg\u00f3ricos que representam palavras diferentes, mas um modelo de linguagem pr\u00e9-treinado n\u00e3o s\u00f3 tem aqueles, mas tamb\u00e9m foi treinado para obter uma representa\u00e7\u00e3o de frases completas e documentos.\n", "", "\u00c9 por isso que a biblioteca est\u00e1 estruturada em torno de tr\u00eas passos:\n", "", "1. obter os seus dados pr\u00e9-processados \u200b\u200be prontos para usar em uma quantidade m\u00ednima de c\u00f3digo,\n", "1. Criar um modelo de linguagem com pesos pr\u00e9-treinado que voc\u00ea pode ajustar para o seu conjunto de dados,\n", "1. Crie outros modelos, como classificadores em cima do codificador do modelo de linguagem.\n", "", "Para mostrar exemplos, n\u00f3s fornecemos uma pequena amostra do [IMDB dataset](https://www.imdb.com/interfaces/) que cont\u00e9m 1.000 coment\u00e1rios de filmes com etiquetas (positivos ou negativos)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["PosixPath('/home/ubuntu/.fastai/data/imdb_sample')"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["path = untar_data(URLs.IMDB_SAMPLE)\n", "path"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Criando um conjunto de dados a partir de seus textos crus \u00e9 muito simples se voc\u00ea t\u00ea-lo em uma dessas maneiras\n", "- organizou em pastas com um estilo IMAGEnet\n", "- organizada em um arquivo CSV com as colunas r\u00f3tulos e um colunas de texto\n", "", "Aqui, a amostra de title est\u00e1 em um arquivos de textos CSV que se parece com isso:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>label</th>\n", "      <th>text</th>\n", "      <th>is_valid</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>negative</td>\n", "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n", "      <td>False</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>positive</td>\n", "      <td>This is a extremely well-made film. The acting...</td>\n", "      <td>False</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>negative</td>\n", "      <td>Every once in a long while a movie will come a...</td>\n", "      <td>False</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>positive</td>\n", "      <td>Name just says it all. I watched this movie wi...</td>\n", "      <td>False</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>negative</td>\n", "      <td>This movie succeeds at being one of the most u...</td>\n", "      <td>False</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["      label                                               text  is_valid\n", "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n", "1  positive  This is a extremely well-made film. The acting...     False\n", "2  negative  Every once in a long while a movie will come a...     False\n", "3  positive  Name just says it all. I watched this movie wi...     False\n", "4  negative  This movie succeeds at being one of the most u...     False"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["df = pd.read_csv(path/'texts.csv')\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Obtendo seus dados prontos para modelagem"]}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [], "source": ["for file in ['train_tok.npy', 'valid_tok.npy']:\n", "    if os.path.exists(path/'tmp'/file): os.remove(path/'tmp'/file)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para obter um [`DataBunch`](/basic_data.html#DataBunch) rapidamente, h\u00e1 tamb\u00e9m v\u00e1rios m\u00e9todos de f\u00e1brica, dependendo de como os nossos dados \u00e9 estruturado. Eles est\u00e3o todos detalhados no [`text.data`](/text.data.html#text.data), aqui vamos usar o m\u00e9todo <code> from_csv </ code> do [`TextLMDataBunch`](/text.data.html#TextLMDataBunch) (para obter os dados prontos para um modelo de linguagem) e [`TextClasDataBunch`](/text.data.html#TextClasDataBunch) (para obter os dados prontos para um classificador de texto) classes ."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Language model data\n", "data_lm = TextLMDataBunch.from_csv(path, 'texts.csv')\n", "# Classifier model data\n", "data_clas = TextClasDataBunch.from_csv(path, 'texts.csv', vocab=data_lm.train_ds.vocab, bs=32)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Isto faz todo o pr\u00e9-processamento necess\u00e1rio por tr\u00e1s da cena. Para o classificador, n\u00f3s tamb\u00e9m passar o vocabul\u00e1rio (mapeamento de ids para palavras) que deseja usar: isso \u00e9 para garantir que `data_clas` usar\u00e1 o mesmo dicion\u00e1rio como` data_lm`.\n", "", "Uma vez que este passo pode ser um pouco demorado, \u00e9 melhor para salvar o resultado com:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_lm.save('data_lm_export.pkl')\n", "data_clas.save('data_clas_export.pkl')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Isto ir\u00e1 criar um diret\u00f3rio 'tmp', onde todo o material computadorizada ser\u00e1 armazenado. Voc\u00ea pode ent\u00e3o recarregar esses resultados com:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_lm = load_data(path, 'data_lm_export.pkl')\n", "data_clas = load_data(path, 'data_clas_export.pkl', bs=16)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note que voc\u00ea pode carregar os dados com diferentes par\u00e2metros [`DataBunch`](/basic_data.html#DataBunch) (tamanho do lote, `bptt`, ...)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ajustar um modelo de linguagem"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Podemos usar o `data_lm` objeto criado anteriormente para afinar um modelo de linguagem pr\u00e9-treinado. [fast.ai](http://www.fast.ai/) tem um modelo de Ingl\u00eas com uma arquitetura AWD-LSTM dispon\u00edvel que pode baixar. Podemos criar um objeto aluno que ir\u00e1 criar diretamente um modelo, baixar os pesos pr\u00e9-treinado e estar pronto para o ajuste fino."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: left;\">\n", "      <th>epoch</th>\n", "      <th>train_loss</th>\n", "      <th>valid_loss</th>\n", "      <th>accuracy</th>\n", "      <th>time</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>0</td>\n", "      <td>4.319174</td>\n", "      <td>3.882361</td>\n", "      <td>0.288155</td>\n", "      <td>00:13</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n", "learn.fit_one_cycle(1, 1e-2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Como um modelo de vis\u00e3o por computador, podemos descongelar o modelo e ajustar-lo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: left;\">\n", "      <th>epoch</th>\n", "      <th>train_loss</th>\n", "      <th>valid_loss</th>\n", "      <th>accuracy</th>\n", "      <th>time</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>0</td>\n", "      <td>3.935607</td>\n", "      <td>3.811058</td>\n", "      <td>0.297262</td>\n", "      <td>00:16</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["learn.unfreeze()\n", "learn.fit_one_cycle(1, 1e-3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para avaliar o seu modelo de linguagem, voc\u00ea pode executar o m\u00e9todo [`Learner.predict`](/basic_train.html#Learner.predict) e especificar o n\u00famero de palavras que voc\u00ea deseja que ele adivinhar."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["'This is a review about what was worth a word of reading , where some'"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["learn.predict(\"This is a review about\", n_words=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["N\u00e3o faz muito sentido (temos uma pequena vocabul\u00e1rio aqui e n\u00e3o treinou muito sobre ele), mas note que respeita gram\u00e1tica b\u00e1sica (que vem do modelo pr\u00e9-treinado).\n", "", "Finalmente, salve o codificador para ser capaz de us\u00e1-lo para a classifica\u00e7\u00e3o na pr\u00f3xima se\u00e7\u00e3o."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learn.save_encoder('ft_enc')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Constru\u00e7\u00e3o de um classificador"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vamos agora usar o `data_clas` objeto criado anteriormente para construir um classificador com nosso codificador afinado. O objeto aluno pode ser feito em uma \u00fanica linha."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n", "learn.load_encoder('ft_enc')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th>text</th>\n", "      <th>target</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n \\n  xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj</td>\n", "      <td>negative</td>\n", "    </tr>\n", "    <tr>\n", "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n", "      <td>positive</td>\n", "    </tr>\n", "    <tr>\n", "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj sydney , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n \\n  xxmaj it 's usually satisfying to watch a film director change his style /</td>\n", "      <td>negative</td>\n", "    </tr>\n", "    <tr>\n", "      <td>xxbos xxmaj this film sat on my xxmaj tivo for weeks before i watched it . i dreaded a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj yorkers . \\n \\n  xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" xxmaj la xxmaj ronde</td>\n", "      <td>positive</td>\n", "    </tr>\n", "    <tr>\n", "      <td>xxbos i really wanted to love this show . i truly , honestly did . \\n \\n  xxmaj for the first time , gay viewers get their own version of the \" xxmaj the xxmaj bachelor \" . xxmaj with the help of his obligatory \" hag \" xxmaj xxunk , xxmaj james , a good looking , well - to - do thirty - something has the chance</td>\n", "      <td>negative</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["data_clas.show_batch()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: left;\">\n", "      <th>epoch</th>\n", "      <th>train_loss</th>\n", "      <th>valid_loss</th>\n", "      <th>accuracy</th>\n", "      <th>time</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>0</td>\n", "      <td>0.603350</td>\n", "      <td>0.531586</td>\n", "      <td>0.741294</td>\n", "      <td>00:23</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["learn.fit_one_cycle(1, 1e-2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Mais uma vez, podemos descongelar o modelo e ajustar-lo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: left;\">\n", "      <th>epoch</th>\n", "      <th>train_loss</th>\n", "      <th>valid_loss</th>\n", "      <th>accuracy</th>\n", "      <th>time</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>0</td>\n", "      <td>0.503833</td>\n", "      <td>0.444084</td>\n", "      <td>0.800995</td>\n", "      <td>00:29</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["learn.freeze_to(-2)\n", "learn.fit_one_cycle(1, slice(5e-3/2., 5e-3))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: left;\">\n", "      <th>epoch</th>\n", "      <th>train_loss</th>\n", "      <th>valid_loss</th>\n", "      <th>accuracy</th>\n", "      <th>time</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>0</td>\n", "      <td>0.410422</td>\n", "      <td>0.363226</td>\n", "      <td>0.850746</td>\n", "      <td>00:42</td>\n", "    </tr>\n", "  </tbody>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["learn.unfreeze()\n", "learn.fit_one_cycle(1, slice(2e-3/100, 2e-3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Mais uma vez, podemos prever em um texto simples usando o m\u00e9todo [`Learner.predict`](/basic_train.html#Learner.predict)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["(Category positive, tensor(1), tensor([0.0049, 0.9951]))"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["learn.predict(\"This was a great movie!\")"]}], "metadata": {"jekyll": {"keywords": "fastai", "summary": "Application to NLP, including ULMFiT fine-tuning", "title": "text"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2}